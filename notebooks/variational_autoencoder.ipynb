{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=10, sharex=False,\n",
    "    sharey=True, figsize=(20, 7))\n",
    "counter = 0\n",
    "for i in range(120, 130):\n",
    "    axes[counter].set_title(y_train[i])\n",
    "    axes[counter].imshow(X_train[i], cmap='gray')\n",
    "    axes[counter].get_xaxis().set_visible(False)\n",
    "    axes[counter].get_yaxis().set_visible(False)\n",
    "    counter += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert from (no_of_data, 28, 28) to (no_of_data, 28, 28, 1)\n",
    "X_train_new = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test_new = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "print(X_train_new.shape)\n",
    "print(X_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width, num_channels = X_train_new.shape[1:]\n",
    "input_shape =  (img_height, img_width, num_channels)\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input = Input(shape=input_shape)\n",
    "\n",
    "encoder_conv = Conv2D(filters=8, kernel_size=3, strides=2,padding='same', activation='relu')(encoder_input)\n",
    "encoder_conv = Conv2D(filters=16, kernel_size=3, strides=2,padding='same', activation='relu')(encoder_conv)\n",
    "\n",
    "encoder = Flatten()(encoder_conv)\n",
    "mu = Dense(latent_dim)(encoder)\n",
    "sigma = Dense(latent_dim)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent(x):\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.int_shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim))\n",
    "    return mu + K.exp(sigma/2)*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latent_space = Lambda(compute_latent, output_shape=(latent_dim,))([mu, sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7, 7, 16)\n"
     ]
    }
   ],
   "source": [
    "conv_shape = K.int_shape(encoder_conv)\n",
    "print(conv_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "decoder = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(decoder)\n",
    "\n",
    "decoder_conv = Conv2DTranspose(filters=16, kernel_size=3, strides=2, padding='same', activation='relu')(decoder)\n",
    "decoder_conv = Conv2DTranspose(filters=8, kernel_size=3, strides=2, padding='same', activation='relu')(decoder_conv)\n",
    "decoder_conv = Conv2DTranspose(filters=num_channels, kernel_size=3, padding='same', activation='sigmoid')(decoder_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 8)    80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 16)     1168        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 784)          0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1570        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            1570        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,388\n",
      "Trainable params: 4,388\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 784)               2352      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         73        \n",
      "=================================================================\n",
      "Total params: 5,905\n",
      "Trainable params: 5,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(encoder_input, latent_space)\n",
    "decoder = Model(decoder_input, decoder_conv)\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 2)                 4388      \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 28, 28, 1)         5905      \n",
      "=================================================================\n",
      "Total params: 10,293\n",
      "Trainable params: 10,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae = Model(encoder_input, decoder(encoder(encoder_input)))\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(true, pred):    # Reconstruction loss\n",
    "    reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height    # KL divergence loss\n",
    "    kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5    # Total loss = 50% rec + 50% KL divergence loss\n",
    "    return K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "vae.compile(optimizer='adam', loss=kl_reconstruction_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/naivoder/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 190.0412 - acc: 0.7986 - val_loss: 172.3737 - val_acc: 0.7868\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 168.7551 - acc: 0.7895 - val_loss: 166.4890 - val_acc: 0.7941\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 164.2463 - acc: 0.7929 - val_loss: 162.7797 - val_acc: 0.7946\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 162.0993 - acc: 0.7940 - val_loss: 161.3341 - val_acc: 0.7960\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 160.9746 - acc: 0.7946 - val_loss: 160.3135 - val_acc: 0.7930\n",
      "Epoch 6/20\n",
      "49760/60000 [=======================>......] - ETA: 2s - loss: 160.4099 - acc: 0.7948"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d42710327863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hd/miniconda3/envs/yolo/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = vae.fit(x=X_train_new, y=X_train_new, epochs=20, batch_size=32, validation_data=(X_test_new,X_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder.predict(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "plt.scatter(encoded[:,0], encoded[:,1], s=2, c=y_train, cmap='hsv')\n",
    "plt.colorbar()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(decoder, n=30, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    scale = 1.0\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent_space(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "yolo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
